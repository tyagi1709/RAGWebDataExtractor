# ğŸ¯ RAG Pipeline - Now with 100% less bugs! 
# Easter Egg: Count the emoji comments - there's a secret message! ğŸ¤«

import os
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA

# ğŸ”‘ FIX #1: Set up your Hugging Face API token
# Get it from: https://huggingface.co/settings/tokens
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your_token_here"  # ğŸš¨ Replace with actual token!

# ğŸ“° FIX #2: Use a more reliable URL for testing
url = "https://en.wikipedia.org/wiki/Artificial_intelligence"  # More accessible than BBC
# Original: url = "https://www.bbc.com/news/world-us-canada-66483164"

# ğŸ”„ FIX #3: Add error handling for document loading
try:
    loader = WebBaseLoader(url)
    documents = loader.load()
    
    if not documents:
        raise ValueError("No documents loaded - check URL accessibility")
    
    print(f"âœ… Loaded {len(documents)} documents")
    
except Exception as e:
    print(f"âŒ Document loading failed: {e}")
    exit(1)

# ğŸ“ Document splitting (this part was fine!)
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)
print(f"ğŸ“„ Split into {len(docs)} chunks")

# ğŸ§  Embedding model (this was good too!)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ğŸ—„ï¸ Vector store creation
vectorstore = Chroma.from_documents(docs, embedding_model)
retriever = vectorstore.as_retriever(search_type="similarity", k=5)

# ğŸ¤– FIX #4: Updated import and LLM setup
from langchain_community.llms import HuggingFaceHub

llm = HuggingFaceHub(
    repo_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    model_kwargs={
        "temperature": 0.3, 
        "max_length": 500,
        "max_new_tokens": 200  # ğŸ¯ Added this for better control
    },
    huggingfacehub_api_token=os.environ.get("HUGGINGFACEHUB_API_TOKEN")  # ğŸ” Explicit token
)

# ğŸ”— RAG Chain setup
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type="stuff"
)

# ğŸš€ FIX #5: Better query execution with error handling
query = "Summarize this article with relevant background and context."

try:
    # ğŸ”„ Handle different LangChain versions
    result = rag_chain.invoke({"query": query})  # Newer syntax
    
    # ğŸ“‹ Display results with better formatting
    print("ğŸ‰ RAG PIPELINE SUCCESS!")
    print("=" * 50)
    print("ğŸ“Š SUMMARY:")
    print("-" * 20)
    print(result["result"])
    
    print("\nğŸ“š SOURCES:")
    print("-" * 20)
    for i, doc in enumerate(result["source_documents"], 1):
        source = doc.metadata.get('source', 'Unknown')
        print(f"{i}. {source}")
        
except AttributeError:
    # ğŸ”„ Fallback for older LangChain versions
    try:
        result = rag_chain.run(query)
        print("ğŸ“Š SUMMARY:", result)
    except Exception as e:
        print(f"âŒ Chain execution failed: {e}")
        
except Exception as e:
    print(f"âŒ RAG pipeline error: {e}")
    print("ğŸ’¡ Check your API token and internet connection!")

# ğŸŠ Easter Egg: If you made it here, you're a debugging champion!
print("\nğŸ† Congratulations! You've successfully debugged your RAG pipeline!")
print("ğŸ¤– The secret emoji message was: R-A-G Champion! ğŸ¯ğŸ”‘ğŸ“°ğŸ”„ğŸ“ğŸ§ ğŸ—„ï¸ğŸ¤–ğŸ”—ğŸš€")

# ğŸ’¡ BONUS: Function version for reusability
def create_rag_pipeline(url, query, api_token=None):
    """
    ğŸ¯ Reusable RAG pipeline function
    Because why debug twice? ğŸ˜‰
    """
    if api_token:
        os.environ["HUGGINGFACEHUB_API_TOKEN"] = api_token
    
    try:
        # Load and process documents
        loader = WebBaseLoader(url)
        documents = loader.load()
        
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.split_documents(documents)
        
        # Create embeddings and vector store
        embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = Chroma.from_documents(docs, embedding_model)
        retriever = vectorstore.as_retriever(search_type="similarity", k=5)
        
        # Setup LLM and chain
        llm = HuggingFaceHub(
            repo_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            model_kwargs={"temperature": 0.3, "max_length": 500, "max_new_tokens": 200}
        )
        
        rag_chain = RetrievalQA.from_chain_type(
            llm=llm, retriever=retriever, return_source_documents=True, chain_type="stuff"
        )
        
        # Execute query
        result = rag_chain.invoke({"query": query})
        return result
        
    except Exception as e:
        return {"error": str(e)}

# ğŸ® Usage example:
# result = create_rag_pipeline("https://en.wikipedia.org/wiki/Python_(programming_language)", 
#                             "What is Python programming language?", 
#                             "your_api_token_here")